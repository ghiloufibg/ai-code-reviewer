apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-worker
  namespace: ai-reviewer
  labels:
    app: llm-worker
    app.kubernetes.io/name: llm-worker
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: ai-code-reviewer
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-worker
  template:
    metadata:
      labels:
        app: llm-worker
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/actuator/prometheus"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
      containers:
        - name: llm-worker
          image: ghcr.io/your-org/ai-code-reviewer/llm-worker:v1.0.0-mvp
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: "prod"
            - name: SPRING_DATA_REDIS_HOST
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: redis.host
            - name: SPRING_DATA_REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: redis.port
            - name: LLM_PROVIDERS_OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: llm-credentials
                  key: openai-api-key
            - name: LLM_PROVIDERS_OPENAI_BASE_URL
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: llm.base-url
            - name: LLM_PROVIDERS_OPENAI_DEFAULT_MODEL
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: llm.model
            - name: SCM_PROVIDERS_GITLAB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: scm-credentials
                  key: gitlab-token
                  optional: true
            - name: SCM_PROVIDERS_GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: scm-credentials
                  key: github-token
                  optional: true
            - name: WORKER_BATCH_SIZE
              value: "10"
            - name: WORKER_TIMEOUT_SECONDS
              value: "120"
            - name: JAVA_OPTS
              value: "-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0 -XX:+UseZGC -XX:+ExitOnOutOfMemoryError"
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "768Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
          volumeMounts:
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: tmp
          emptyDir: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: llm-worker
                topologyKey: kubernetes.io/hostname
